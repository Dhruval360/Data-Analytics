{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-6rz1kkSboN"
      },
      "source": [
        "# Exploratory Data Analysis of the DengAI dataset\n",
        "\n",
        "## Sections\n",
        "1. [Data Ingestion](#Data_Ingestion)\n",
        "2. [Summary Statistics](#Summary_Statistics)\n",
        "3. Data Cleaning and Preprocessing\n",
        "4. [Visualization](#Visualization)\n",
        "5. Performing different tests {Correlation analysis, Dickey Fuller test, ADF, etc.}\n",
        "6. [Conclusions](#Conclusions)\n",
        "\n",
        "7. Experimenting with different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ9y0KNHMa7O"
      },
      "source": [
        "## Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyGyszbS_IHH",
        "outputId": "f44030f0-c87d-4c86-dc18-a9a45c801b8a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose as decompose\n",
        "from pandas import Series\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime as dt\n",
        "from matplotlib.pyplot import figure\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJGW-NQaNn1M"
      },
      "source": [
        "***\n",
        "<a id='Data_Ingestion'></a>\n",
        "# 1. Data Ingestion\n",
        "\n",
        "The study consists of data for two cities: __*San Juan*__ and __*Iquitos*__\n",
        "\n",
        "The dataset consists of two parts\n",
        "* __dengue_features_train.csv__ : Contains readings for all the factors considered in the study (like humidity, temperature, etc.)\n",
        "* __dengue_labels_train.csv__ : Contains the number of Dengue cases reported on a weekly basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "MbLR09w8pePu",
        "outputId": "17293e6d-cb04-4499-e038-e35d61aec44b"
      },
      "outputs": [],
      "source": [
        "env = 'VSCode'#'Colab'\n",
        "path = '.'\n",
        "\n",
        "if env == 'Colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !ls drive/MyDrive/Dataset\n",
        "    path = 'drive/MyDrive'\n",
        "\n",
        "features = pd.read_csv(f'{path}/Dataset/dengue_features_train.csv')\n",
        "features.week_start_date = pd.to_datetime(features.week_start_date) # Converting the date into a date time object\n",
        "\n",
        "cases = pd.read_csv(f'{path}/Dataset/dengue_labels_train.csv')\n",
        "testDf = pd.read_csv(f'{path}/Dataset/dengue_features_test.csv')\n",
        "\n",
        "# Performing an Inner Join on the dataset on the common columns\n",
        "df = pd.merge(features, cases, on = [\"year\", \"weekofyear\", \"city\"])\n",
        "df.index = df.week_start_date\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBfUjqn9SboZ"
      },
      "source": [
        "## Counting number of rows and columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12o7DPfLSboa",
        "outputId": "da9dc789-d4d5-4ec2-f19c-18bad77ed8e4"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of attributes: {len(df.columns)}\")\n",
        "print(f\"Number of rows {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acpyLIH5P9tb"
      },
      "source": [
        "## As our dataset consists of two different cities, we will perform analysis on each city separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "LP1jLWLswZZC",
        "outputId": "0f828f45-e8f1-44e4-bb9b-f7ec59517bd5"
      },
      "outputs": [],
      "source": [
        "sj_df = df[df[\"city\"] == 'sj']\n",
        "sj_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "ucIk8TZ7zbEv",
        "outputId": "bd3ba562-6952-4726-f568-e8606919daf1"
      },
      "outputs": [],
      "source": [
        "iq_df = df[df[\"city\"] == 'iq']\n",
        "iq_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcR1HxxnSboc",
        "outputId": "3e52b65e-8f7d-4888-8331-fba74f4ac28c"
      },
      "outputs": [],
      "source": [
        "datasets = {\"San Juan\" : sj_df, \"Iquitos\" : iq_df}\n",
        "for city in datasets:\n",
        "    print(f\"Shape of {city}'s data = {datasets[city].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyMkSP67Sbod"
      },
      "source": [
        "***\n",
        "<a id='Summary_Statistics'></a>\n",
        "# 2. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Kf2ZpqgESbod",
        "outputId": "00b78b01-9605-4920-ef0b-9699df0c6845"
      },
      "outputs": [],
      "source": [
        "sj_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "VG58SbkeSboe",
        "outputId": "911c9dc5-9620-47fd-ea71-8dff4f4455b6"
      },
      "outputs": [],
      "source": [
        "iq_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyxiroNiOwvF"
      },
      "source": [
        "## 2.1 Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63pyHIpBkf3K",
        "outputId": "5d01c27f-0386-4e0f-c054-ad45f2f66b8c"
      },
      "outputs": [],
      "source": [
        "for city in datasets:\n",
        "    print(city)\n",
        "    nullVals = datasets[city].isnull()\n",
        "    nullValCounts = nullVals.sum()\n",
        "    print(dict(nullValCounts))\n",
        "    print(f\"\\nFraction of data missing per column:\\n{dict(nullValCounts / len(datasets[city]))}\\n\")\n",
        "    print(f\"Total Missing data = {nullValCounts.sum()}\")\n",
        "    print(f\"Total Duplicated data = {datasets[city].duplicated().sum()}\")\n",
        "    print(f\"Total Incomplete data = {nullVals.any(axis = 1).sum()}\")\n",
        "    print(\"------------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfR59AdySnn2"
      },
      "source": [
        "## 2.2 Outliers\n",
        "Box-Plots to visualize outliers in the number of cases for both the cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bISsHpTNUsB"
      },
      "outputs": [],
      "source": [
        "# Box plot for every year\n",
        "plt.rcParams[\"figure.figsize\"] = 24, 15\n",
        "fig, axs = plt.subplots(2, 1)\n",
        "\n",
        "# Number of cases every year\n",
        "sj_df.boxplot(by = 'year', column = ['total_cases'], ax = axs[0])\n",
        "axs[0].set_title(\"Yearly cases in San Juan\", size = 24)\n",
        "\n",
        "iq_df.boxplot(by = \"year\", column = [\"total_cases\"], ax = axs[1])\n",
        "axs[1].set_title(\"Yearly cases in Iquitos\", size = 24)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVZz6Oo9nXbc",
        "outputId": "a19d6951-35ce-437e-ebef-6e8246c0efe8"
      },
      "outputs": [],
      "source": [
        "sj_df.insert(loc = len(sj_df.columns), column = 'month', value = sj_df['week_start_date'].dt.strftime(\"%b\"))\n",
        "year_monthly = sj_df.groupby(['year','month']).sum('total_cases')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEUWlHRoqQFd",
        "outputId": "c0d9ad15-b1bc-4722-ad26-80ad096d8b25"
      },
      "outputs": [],
      "source": [
        "year_monthly.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "B9ZP4odLqTzO",
        "outputId": "18e39484-37d3-433c-d3f0-d934aacaf521"
      },
      "outputs": [],
      "source": [
        "year_monthly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "eyh94N7_pXra",
        "outputId": "569e9855-988d-45c6-ed33-3f1c6a817717"
      },
      "outputs": [],
      "source": [
        "# fig, ax = plt.subplots(figsize=(8,6))\n",
        "# year_monthly['total_cases'].plot(kind='kde',ax=ax)\n",
        "\n",
        "year_monthly.unstack('month')['total_cases'].plot(kind = \"bar\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmKaeZ_2qrQp",
        "outputId": "f35efe3e-c154-4b1e-db25-fd6a94cde75e"
      },
      "outputs": [],
      "source": [
        "print(year_monthly['total_cases'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpRLL5VQSboi"
      },
      "source": [
        "***\n",
        "<a id='Visualization'></a>\n",
        "# 4. Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvM5lnXSboj"
      },
      "source": [
        "## 4.1 Line Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P2_jGKhSboj"
      },
      "source": [
        "### 4.1.1 Line plot to vizualize `Number of cases vs time`, `Humidity vs time` and `temperature vs time`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1JqXchaY-7XD",
        "outputId": "9cc6f480-8c8e-4912-abd9-4787bb9b575a"
      },
      "outputs": [],
      "source": [
        "all_concerns = [\n",
        "  'total_cases',\n",
        "  'reanalysis_specific_humidity_g_per_kg',\n",
        "  'reanalysis_avg_temp_k'\n",
        "]\n",
        "\n",
        "labels = [\n",
        "  'Dengue Cases',\n",
        "  'Humidity',\n",
        "  'Average Temperature'\n",
        "]\n",
        "\n",
        "colors = [\"red\", \"green\"]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (27, 27) # rc -> runtime config\n",
        "fig,ax = plt.subplots(len(all_concerns) * len(datasets), 1) \n",
        "\n",
        "for index, city in enumerate(datasets):\n",
        "  for i,concern in enumerate(all_concerns):\n",
        "    ax[i + index*len(all_concerns)].plot(\n",
        "        datasets[city].index,\n",
        "        datasets[city][concern],\n",
        "        color = colors[0 + index],\n",
        "        label = f\"{labels[i]} in {city}\"\n",
        "    )\n",
        "    ax[i + index*len(all_concerns)].set_title(f\"{labels[i]} vs time\")\n",
        "    ax[i + index*len(all_concerns)].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FofceWhNScN3"
      },
      "source": [
        "### 4.1.2 Finding the number of yearly cases in both cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "anjaJ1GU7bIS",
        "outputId": "5c49747d-683d-47ed-8a38-4eda0e93850c"
      },
      "outputs": [],
      "source": [
        "sj_df_yearly = pd.DataFrame(\n",
        "    {\n",
        "        \"year\" : np.unique(sj_df['year']),\n",
        "        \"total_cases\" : sj_df.groupby([\"year\"])[\"total_cases\"].sum()\n",
        "    }\n",
        ")\n",
        "\n",
        "iq_df_yearly = pd.DataFrame(\n",
        "    {\n",
        "        \"year\" : np.unique(iq_df['year']),\n",
        "        \"total_cases\" : iq_df.groupby([\"year\"])[\"total_cases\"].sum()\n",
        "    }\n",
        ")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (25, 7) #rc -> runtime config\n",
        "fig, axs = plt.subplots(1, 2) \n",
        "\n",
        "axs[0].plot(sj_df_yearly.year, sj_df_yearly.total_cases)\n",
        "axs[0].set_title(\"Yearly Cases in San Juan\")\n",
        "\n",
        "axs[1].plot(iq_df_yearly.year, iq_df_yearly.total_cases)\n",
        "axs[1].set_title(\"Yearly Cases in Iquitos\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tyjLeJAS5oM"
      },
      "source": [
        "## 4.2 Histograms\n",
        "Understanding the distribution of data for `total number of cases, specific humidity and air temperature in both cities`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8IYx3dTgh00"
      },
      "source": [
        "**TODO FOR ALL IMPORTANT FEATURES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DN_tNFw7Z-MU",
        "outputId": "ee3428c6-c18c-4642-8507-701c9f85340e"
      },
      "outputs": [],
      "source": [
        "all_concerns = [\n",
        "  'total_cases',\n",
        "  'reanalysis_specific_humidity_g_per_kg',\n",
        "  'reanalysis_avg_temp_k'\n",
        "]\n",
        "\n",
        "labels = [\n",
        "  'Dengue Cases',\n",
        "  'Humidity',\n",
        "  'Average Temperature'\n",
        "]\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = 30, 20\n",
        "fig, axs = plt.subplots(len(all_concerns), len(datasets)) \n",
        "plt.subplots_adjust(hspace = 0.8) #space between plots\n",
        "\n",
        "for index, city in enumerate(datasets):\n",
        "    for i,concern in enumerate(all_concerns):\n",
        "        sns.histplot(datasets[city][concern], ax = axs[i, index], kde = True)\n",
        "        axs[i, index].set_title(f\"{labels[i]} in {city}\",size=14)\n",
        "        \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faWJ7-nvfl5B"
      },
      "source": [
        "The Dengue cases in San Juan and Iquitos are both right skewed.\n",
        "\n",
        "Humidity is almost right left skewed while the average temperature are subbotin distribution due to the plateu in between"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUTPm7MFSbom"
      },
      "source": [
        "## 4.3 Correlation matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kb3C4R5l5sD5",
        "outputId": "0bd0812c-28e8-49d6-c13e-ca710745c96c"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 10))\n",
        "corr_cases = sj_df.corr()\n",
        "sns.heatmap(corr_cases, square = True, cmap = 'coolwarm', ax = ax)\n",
        "plt.title('Correlation Matrix for San Juan', fontsize = 16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3wABABPZSbon",
        "outputId": "a89b4241-2757-4e17-bcc4-84cbef7b7c84"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 10))\n",
        "corr_cases = iq_df.corr()\n",
        "sns.heatmap(corr_cases, square = True, cmap = 'coolwarm', ax = ax)\n",
        "plt.title('Correlation Matrix for Iquitos', fontsize = 16)\n",
        "plt.rcParams['figure.figsize'] = (5, 5) #rc -> runtime config\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rL272DSSbon"
      },
      "source": [
        "***\n",
        "<a id='Conclusions'></a>\n",
        "# 6. Conclusions\n",
        "\n",
        "- How many rows and attributes?\n",
        "    - San Juan: `(936, 25)`.\n",
        "    - Iquitos : `(520, 25)`.\n",
        "- How many missing data and outliers?\n",
        "    - San Juan: `380 Missing`.\n",
        "    - Iquitos : `168 Missing`.\n",
        "- Any inconsistent, incomplete, duplicate or incorrect data?\n",
        "    - All records in both the datasets are unique.\n",
        "    - Number of incomplete rows in San Juan's data: `209`.\n",
        "    - Number of incomplete rows in Iquitos's data : `48`.\n",
        "- Are the variables correlated to each other?\n",
        "    - San Juan: ``.\n",
        "    - Iquitos : ``.\n",
        "\n",
        "- Are any of the preprocessing techniques needed: rolling average, continuum cubic spline curve, dimensionality reduction, range transformation, standardization, etc.?\n",
        "    - San Juan: ``.\n",
        "    - Iquitos : ``.\n",
        "    \n",
        "- Does PCA help visualize the data? Do we get any insights from histograms/bar charts/line plots, etc.?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgZQYNyDT2oY",
        "outputId": "e5dc49d2-802d-4881-bd15-d28bcf2029de"
      },
      "outputs": [],
      "source": [
        "def imputeMissing(dfs):\n",
        "    for adf in dfs:\n",
        "        for col in adf.columns[adf.isna().any()].tolist():\n",
        "            adf[col].interpolate(method=\"time\",inplace=True) # Why inplace?\n",
        "    return iq_df,sj_df\n",
        "\n",
        "iq_df_imputed, sj_df_imputed = imputeMissing([iq_df,sj_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sj_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "12jh9uTmTsTG",
        "outputId": "71780f34-e1c4-43eb-983c-0b875e22d0c4"
      },
      "outputs": [],
      "source": [
        "def pcaAnalysis(dfs_list,num_components): #work on the imputed data\n",
        "    dfs,original_dfs = dfs_list\n",
        "\n",
        "    details = {\n",
        "        0:['iq','red'],\n",
        "        1:['sj','blue']\n",
        "    }\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    pca = PCA()\n",
        "    res = []\n",
        "    \n",
        "    #number of components to keep can be explained with the help of \n",
        "    #explained variance ratio as a function of the number of components\n",
        "    #it is the percentage of variance attributed to each of the selected component\n",
        "    #hence we shld go on adding components until the total variance ratio is upto 80% or so to avoid overfitting\n",
        "\n",
        "    for df_indx,adf in enumerate(dfs):\n",
        "        adf = adf.drop(['city', 'month'], axis = 1, errors = 'ignore')\n",
        "        \n",
        "        #convert datetime to ordinal value from datetime object for normalization\n",
        "        adf['week_start_date'] = adf['week_start_date'].map(dt.datetime.toordinal)\n",
        "        adf = sc.fit_transform(adf)\n",
        "        adf_pca = pca.fit(adf)\n",
        "        \n",
        "        plt.plot(\n",
        "            np.cumsum(pca.explained_variance_ratio_),\n",
        "            label = details[df_indx][0],\n",
        "            color = details[df_indx][1],\n",
        "        )\n",
        "\n",
        "        pca = PCA(num_components)\n",
        "        adf_pca = pca.fit_transform(adf)\n",
        "\n",
        "        #get the most important features\n",
        "        #pca.components_ is a list of eigenvector magnitudes for each priciple component\n",
        "        most_imp = [np.abs(pca.components_[i]).argmax() for i in range(pca.n_components_)]\n",
        "        most_imp_names = [original_dfs[df_indx].columns[most_imp[i]] for i in range(pca.n_components_)]\n",
        "        res.append(most_imp_names)\n",
        "\n",
        "    plt.xlabel(\"number of components\")\n",
        "    plt.ylabel('cumulative explained variance')\n",
        "    plt.legend()\n",
        "    plt.plot()\n",
        "    return res\n",
        "\n",
        "important_columns = pcaAnalysis([[iq_df_imputed,sj_df_imputed],[iq_df,sj_df]],15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTJkySvEkd4P"
      },
      "outputs": [],
      "source": [
        "def getCorrelationAtWeeksLag(df,max_lag):\n",
        "    target = 'total_cases'\n",
        "    df = df.drop(['city','year','week_start_date','weekofyear'], axis = 1)\n",
        "    lagged_correlation = pd.DataFrame.from_dict(\n",
        "    {x: [df[target].corr(df[x].shift(-t)) for t in range(max_lag+1)] for x in df.columns})\n",
        "    return lagged_correlation,max_lag\n",
        "\n",
        "\n",
        "lagged_correlation,max_lag = getCorrelationAtWeeksLag(iq_df,28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "eBMQF1SDlKdc",
        "outputId": "5e08a693-d01e-4e47-ea7f-35b353297c90"
      },
      "outputs": [],
      "source": [
        "lagged_correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SCdpQjkJoqn0",
        "outputId": "32056e46-8ebe-434d-f6c2-1f5b04e69ab3"
      },
      "outputs": [],
      "source": [
        "def plotCorrelationAtLags(lagged_correlation,max_lag):\n",
        "    plt.rcParams[\"figure.figsize\"] = (20,20)\n",
        "\n",
        "    for col in lagged_correlation.columns:\n",
        "        if col!='total_cases':\n",
        "            y= lagged_correlation[col]\n",
        "            x = range(0,max_lag+1)\n",
        "            plt.plot(x,y,label=f'{col}')\n",
        "            plt.legend()\n",
        "            plt.xlabel(\"Lag weeks\")\n",
        "            plt.ylabel(\"Correlation value with total_cases\")\n",
        "    plt.show()\n",
        "\n",
        "plotCorrelationAtLags(lagged_correlation,max_lag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcnrRlT-B4Nt"
      },
      "source": [
        "Seasonality Decomposition "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pvQBxR8K8oVp",
        "outputId": "d1593d1c-25ab-475b-8f1b-11813d15fe3f"
      },
      "outputs": [],
      "source": [
        "res = seasonal_decompose(iq_df_imputed['total_cases'], model=\"additive\", period=4)\n",
        "res.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJnUvbtBk-7z"
      },
      "source": [
        "Inference \n",
        "->Trend : slightly increasing trend\n",
        "-> Has seasonality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from xgboost import XGBRegressor, plot_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainDfiq = iq_df_imputed\n",
        "trainDfsj = sj_df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(trainDf):\n",
        "    unwantedCols = ['month', 'week_start_date', 'year', 'weekofyear', 'week_start_date', 'city', 'total_cases']#, 'Volume']\n",
        "\n",
        "    x = trainDf.drop(unwantedCols, axis = 1, errors = 'ignore')\n",
        "    y = trainDf['total_cases']\n",
        "\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    # Parameters for Grid Search\n",
        "    param_grid = dict(\n",
        "        preprocessor = [MinMaxScaler(), RobustScaler(), StandardScaler()]\n",
        "    )\n",
        "\n",
        "    xgbModel = XGBRegressor(\n",
        "        # booster = 'gblinear',\n",
        "        # objective ='reg:squarederror',\n",
        "        random_state = 1, \n",
        "        n_estimators = 500, \n",
        "        learning_rate = 0.1\n",
        "    )\n",
        "\n",
        "    xgbPipeline = Pipeline(\n",
        "        steps = [\n",
        "            ('preprocessor', RobustScaler()),\n",
        "            ('model', tunedXGB)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    grid = GridSearchCV(xgbPipeline, param_grid = param_grid, cv = 4, verbose = 3)\n",
        "    grid.fit(x, y)\n",
        "    print(\"\\nBest Model:\")\n",
        "    print(grid.best_estimator_)\n",
        "\n",
        "    xgbPipeline = grid.best_estimator_\n",
        "\n",
        "    scores = -1 * cross_val_score(xgbPipeline, x, y, cv = 5, scoring = 'neg_mean_absolute_error', verbose = 1)\n",
        "    print(f'Mean MAE: {scores.mean()} ({scores.std()})')\n",
        "\n",
        "    print(f\"Training Score  : {xgbPipeline.score(x_train, y_train)}\")\n",
        "    print(f\"Validation Score: {xgbPipeline.score(x_valid, y_valid)}\")\n",
        "\n",
        "    print(x.columns)\n",
        "    xgbPipeline.steps[1][1].get_booster().feature_names = list(x.columns)\n",
        "    plot_importance(xgbPipeline.steps[1][1].get_booster())\n",
        "    plt.show()\n",
        "\n",
        "    return xgbPipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sjModel = train(sj_df_imputed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iqModel = train(iq_df_imputed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicting on the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testSJ = testDf[testDf.city == 'sj'].drop(unwantedCols, axis = 1, errors = 'ignore')\n",
        "testIQ = testDf[testDf.city == 'iq'].drop(unwantedCols, axis = 1, errors = 'ignore')\n",
        "\n",
        "SJPreds = testDf[testDf.city == 'sj'][['city', 'year', 'weekofyear']]\n",
        "IQPreds = testDf[testDf.city == 'iq'][['city', 'year', 'weekofyear']]\n",
        "\n",
        "def predict(df, outputFile, sjModel, iqModel):\n",
        "    SJPreds['total_cases'] = sjModel.predict(testSJ).astype(int)\n",
        "    IQPreds['total_cases'] = iqModel.predict(testIQ).astype(int)\n",
        "\n",
        "    concatenated = pd.concat((SJPreds, IQPreds))\n",
        "    concatenated.to_csv(f'{path}/Predictions/{outputFile}.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EDA_Colab.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "96287154c02bd33d964264908876ff144eece4cbcf76f7700a55934c72d2f956"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
